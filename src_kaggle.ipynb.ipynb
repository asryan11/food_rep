{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11822130,"sourceType":"datasetVersion","datasetId":7426096},{"sourceId":11841462,"sourceType":"datasetVersion","datasetId":7439900},{"sourceId":11841567,"sourceType":"datasetVersion","datasetId":7439965},{"sourceId":11862382,"sourceType":"datasetVersion","datasetId":7454152}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"9e939283","cell_type":"code","source":"%pip install numpy pandas scikit-learn tensorflow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T22:03:27.870421Z","iopub.execute_input":"2025-05-20T22:03:27.870774Z","iopub.status.idle":"2025-05-20T22:03:31.868081Z","shell.execute_reply.started":"2025-05-20T22:03:27.870752Z","shell.execute_reply":"2025-05-20T22:03:31.866776Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.72.0rc1)\nRequirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\nRequirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\nRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.0.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\nRequirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":45},{"id":"dc89318b","cell_type":"code","source":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Embedding\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport pickle","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T22:03:31.870635Z","iopub.execute_input":"2025-05-20T22:03:31.870972Z","iopub.status.idle":"2025-05-20T22:03:31.878538Z","shell.execute_reply.started":"2025-05-20T22:03:31.870943Z","shell.execute_reply":"2025-05-20T22:03:31.876991Z"}},"outputs":[],"execution_count":46},{"id":"6450b5c7","cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/dataset/Cleaned_Indian_Food_Dataset.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T22:03:31.879940Z","iopub.execute_input":"2025-05-20T22:03:31.880348Z","iopub.status.idle":"2025-05-20T22:03:32.116068Z","shell.execute_reply.started":"2025-05-20T22:03:31.880321Z","shell.execute_reply":"2025-05-20T22:03:32.115289Z"}},"outputs":[],"execution_count":47},{"id":"cc8d1083","cell_type":"code","source":"# Preprocessing function (optional but recommended)\ndef preprocess_text(text):\n    text = str(text).lower().strip()       # Ensure string, lowercase, and strip\n    text = ' '.join(text.split())          # Remove extra whitespace\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T22:03:32.118534Z","iopub.execute_input":"2025-05-20T22:03:32.118818Z","iopub.status.idle":"2025-05-20T22:03:32.124213Z","shell.execute_reply.started":"2025-05-20T22:03:32.118798Z","shell.execute_reply":"2025-05-20T22:03:32.123157Z"}},"outputs":[],"execution_count":48},{"id":"22464759-9c30-4009-831d-1b8dc0fef6c8","cell_type":"code","source":"# Apply preprocessing\ninput_texts = [preprocess_text(text) for text in df[\"Cleaned-Ingredients\"]]\ntarget_texts = [\"<start> \" + preprocess_text(text) + \" <end>\" for text in df[\"TranslatedInstructions\"]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T22:03:32.125353Z","iopub.execute_input":"2025-05-20T22:03:32.125664Z","iopub.status.idle":"2025-05-20T22:03:32.323785Z","shell.execute_reply.started":"2025-05-20T22:03:32.125646Z","shell.execute_reply":"2025-05-20T22:03:32.322698Z"}},"outputs":[],"execution_count":49},{"id":"934852c6","cell_type":"code","source":"encoder_tokenizer = Tokenizer()\nencoder_tokenizer.fit_on_texts(input_texts)\nencoder_sequences = encoder_tokenizer.texts_to_sequences(input_texts)\nencoder_input_data = pad_sequences(encoder_sequences, padding='post')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T22:03:32.325267Z","iopub.execute_input":"2025-05-20T22:03:32.325616Z","iopub.status.idle":"2025-05-20T22:03:32.577066Z","shell.execute_reply.started":"2025-05-20T22:03:32.325592Z","shell.execute_reply":"2025-05-20T22:03:32.576241Z"}},"outputs":[],"execution_count":50},{"id":"ad4e2af2","cell_type":"code","source":"decoder_tokenizer = Tokenizer(filters='')\ndecoder_tokenizer.fit_on_texts(target_texts)\nreverse_decoder_word_index = {index: word for word, index in decoder_tokenizer.word_index.items()}\ndecoder_sequences = decoder_tokenizer.texts_to_sequences(target_texts)\ndecoder_input_data = pad_sequences([seq[:-1] for seq in decoder_sequences], padding='post')\ndecoder_target_data = pad_sequences([seq[1:] for seq in decoder_sequences], padding='post')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T22:03:32.578340Z","iopub.execute_input":"2025-05-20T22:03:32.578682Z","iopub.status.idle":"2025-05-20T22:03:34.243947Z","shell.execute_reply.started":"2025-05-20T22:03:32.578660Z","shell.execute_reply":"2025-05-20T22:03:34.242900Z"}},"outputs":[],"execution_count":51},{"id":"ae120001-56eb-422e-a2bb-9cf93a7173ca","cell_type":"code","source":"import pickle\n\n# Load encoder tokenizer\nwith open('/kaggle/input/final-food-rep/encoder_tokenizer.pkl', 'rb') as f:\n    encoder_tokenizer = pickle.load(f)\n\n# Load decoder tokenizer\nwith open('/kaggle/input/final-food-rep/decoder_tokenizer.pkl', 'rb') as f:\n    decoder_tokenizer = pickle.load(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T22:03:34.245192Z","iopub.execute_input":"2025-05-20T22:03:34.245617Z","iopub.status.idle":"2025-05-20T22:03:34.299855Z","shell.execute_reply.started":"2025-05-20T22:03:34.245591Z","shell.execute_reply":"2025-05-20T22:03:34.298745Z"}},"outputs":[],"execution_count":52},{"id":"ec6a2354-4a35-4421-9af7-8fc5c97d959d","cell_type":"code","source":"encoder_vocab_size = len(encoder_tokenizer.word_index) + 1\ndecoder_vocab_size = len(decoder_tokenizer.word_index) + 1\nembedding_dim = 128\nlatent_dim = 256","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T22:03:34.301068Z","iopub.execute_input":"2025-05-20T22:03:34.301494Z","iopub.status.idle":"2025-05-20T22:03:34.308195Z","shell.execute_reply.started":"2025-05-20T22:03:34.301462Z","shell.execute_reply":"2025-05-20T22:03:34.306872Z"}},"outputs":[],"execution_count":53},{"id":"25eaeeb0-b3bb-4a84-84af-e8d102a5d107","cell_type":"code","source":"encoder_inputs = Input(shape=(None,))\nenc_emb = Embedding(encoder_vocab_size, embedding_dim)(encoder_inputs)\nencoder_lstm = LSTM(latent_dim, return_state=True)\n_, state_h, state_c = encoder_lstm(enc_emb)\nencoder_states = [state_h, state_c]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T22:03:34.312375Z","iopub.execute_input":"2025-05-20T22:03:34.312783Z","iopub.status.idle":"2025-05-20T22:03:34.400679Z","shell.execute_reply.started":"2025-05-20T22:03:34.312758Z","shell.execute_reply":"2025-05-20T22:03:34.399547Z"}},"outputs":[],"execution_count":54},{"id":"ab8a4ac8-1683-4cef-8c4c-c5636eb04480","cell_type":"code","source":"decoder_inputs = Input(shape=(None,))\ndec_emb_layer = Embedding(decoder_vocab_size, embedding_dim)\ndec_emb = dec_emb_layer(decoder_inputs)\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\ndecoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\ndecoder_dense = Dense(decoder_vocab_size, activation='softmax')\ndecoder_outputs = decoder_dense(decoder_outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T22:03:34.401537Z","iopub.execute_input":"2025-05-20T22:03:34.401803Z","iopub.status.idle":"2025-05-20T22:03:34.564727Z","shell.execute_reply.started":"2025-05-20T22:03:34.401784Z","shell.execute_reply":"2025-05-20T22:03:34.563770Z"}},"outputs":[],"execution_count":55},{"id":"8fb13baf-d98d-4112-8b61-68e1a6ad0021","cell_type":"code","source":"# model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n# model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T22:03:34.565900Z","iopub.execute_input":"2025-05-20T22:03:34.566293Z","iopub.status.idle":"2025-05-20T22:03:34.571719Z","shell.execute_reply.started":"2025-05-20T22:03:34.566262Z","shell.execute_reply":"2025-05-20T22:03:34.570652Z"}},"outputs":[],"execution_count":56},{"id":"08a640a1","cell_type":"code","source":"# from keras.callbacks import ModelCheckpoint\n\n# checkpoint = ModelCheckpoint(\n#     filepath='/kaggle/working/model_check.weights.h5',  # or .h5 or .weights.h5\n#     save_weights_only=True,\n#     save_best_only=False,\n#     save_freq='epoch',\n#     verbose=1\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T22:03:34.572887Z","iopub.execute_input":"2025-05-20T22:03:34.573133Z","iopub.status.idle":"2025-05-20T22:03:34.591530Z","shell.execute_reply.started":"2025-05-20T22:03:34.573115Z","shell.execute_reply":"2025-05-20T22:03:34.590451Z"}},"outputs":[],"execution_count":57},{"id":"a78413c5-f48f-4fda-b057-7ad17ff3a0dc","cell_type":"code","source":"# Remove all extra dimensions\ndecoder_target_data = np.squeeze(decoder_target_data)\n\n# Add just one last dimension\ndecoder_target_data = np.expand_dims(decoder_target_data, -1)\n\n# Confirm final shape\nprint(decoder_target_data.shape)  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T22:03:34.592917Z","iopub.execute_input":"2025-05-20T22:03:34.593239Z","iopub.status.idle":"2025-05-20T22:03:34.611900Z","shell.execute_reply.started":"2025-05-20T22:03:34.593219Z","shell.execute_reply":"2025-05-20T22:03:34.610694Z"}},"outputs":[{"name":"stdout","text":"(5938, 1005, 1)\n","output_type":"stream"}],"execution_count":58},{"id":"e79407ed-c435-4966-a385-c7c7e3144d5a","cell_type":"code","source":"# from keras.callbacks import EarlyStopping\n\n# early_stopping = EarlyStopping(\n#     monitor='val_loss',\n#     patience=3,  # stops if val_loss doesn't improve for 3 consecutive epochs\n#     restore_best_weights=True\n# )\n\n# model.fit(\n#     [encoder_input_data, decoder_input_data],\n#     decoder_target_data,\n#     batch_size=16,\n#     epochs=50,\n#     validation_split=0.2,\n#     callbacks=[early_stopping]\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T22:03:34.613528Z","iopub.execute_input":"2025-05-20T22:03:34.613796Z","iopub.status.idle":"2025-05-20T22:03:34.633733Z","shell.execute_reply.started":"2025-05-20T22:03:34.613778Z","shell.execute_reply":"2025-05-20T22:03:34.632648Z"}},"outputs":[],"execution_count":59},{"id":"aaab62b3-4663-4fe1-86d9-2dbfc18dee70","cell_type":"code","source":"\nfrom tensorflow.keras.models import load_model\n# model.save('/kaggle/working/seq2seq_model.h5')\nmodel= load_model(\"/kaggle/input/final-food-rep/seq2seq_model.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T22:03:34.635238Z","iopub.execute_input":"2025-05-20T22:03:34.635659Z","iopub.status.idle":"2025-05-20T22:03:36.093343Z","shell.execute_reply.started":"2025-05-20T22:03:34.635626Z","shell.execute_reply":"2025-05-20T22:03:36.092466Z"}},"outputs":[],"execution_count":60},{"id":"c3015a93-1557-4845-b269-12fc8a0b20a8","cell_type":"code","source":"# Encoder inference model\nencoder_model = Model(encoder_inputs, encoder_states)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T22:03:36.094571Z","iopub.execute_input":"2025-05-20T22:03:36.094904Z","iopub.status.idle":"2025-05-20T22:03:36.102274Z","shell.execute_reply.started":"2025-05-20T22:03:36.094881Z","shell.execute_reply":"2025-05-20T22:03:36.100989Z"}},"outputs":[],"execution_count":61},{"id":"b7549616-e387-451c-8b24-ff32153b55fe","cell_type":"code","source":"# Decoder inference model\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\ndec_emb2 = dec_emb_layer(decoder_inputs)\ndecoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\ndecoder_states2 = [state_h2, state_c2]\ndecoder_outputs2 = decoder_dense(decoder_outputs2)\n\ndecoder_model = Model(\n    [decoder_inputs] + decoder_states_inputs,\n    [decoder_outputs2] + decoder_states2\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T22:03:36.103317Z","iopub.execute_input":"2025-05-20T22:03:36.103650Z","iopub.status.idle":"2025-05-20T22:03:36.127282Z","shell.execute_reply.started":"2025-05-20T22:03:36.103619Z","shell.execute_reply":"2025-05-20T22:03:36.125923Z"}},"outputs":[],"execution_count":62},{"id":"200681a2-8437-4fa1-8999-83c7a365bd68","cell_type":"code","source":"def decode_sequence(input_seq):\n    # Encode the input sequence\n    states_value = encoder_model.predict(input_seq)\n\n    # Initialize the target sequence with the <start> token\n    target_seq = np.array([[decoder_tokenizer.word_index['<start>']]])\n\n\n    stop_condition = False\n    decoded_tokens = []\n    max_target_len = 100  # You can increase this\n\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n\n        # Get the token with the highest probability\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_token = decoder_tokenizer.index_word.get(sampled_token_index, '')\n\n        # Stop if <end> token or max length is reached\n        if sampled_token == '<end>' or len(decoded_tokens) > max_target_len:\n            stop_condition = True\n        else:\n            if len(decoded_tokens) == 0 or sampled_token != decoded_tokens[-1]:  # Prevent repetition\n                decoded_tokens.append(sampled_token)\n\n            # Update target sequence and states\n            target_seq = np.array([[sampled_token_index]])\n            states_value = [h, c]\n            \n\n    return ' '.join(decoded_tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T22:03:36.128353Z","iopub.execute_input":"2025-05-20T22:03:36.128684Z","iopub.status.idle":"2025-05-20T22:03:36.150309Z","shell.execute_reply.started":"2025-05-20T22:03:36.128662Z","shell.execute_reply":"2025-05-20T22:03:36.149231Z"}},"outputs":[],"execution_count":63},{"id":"51c75531-09a2-4192-98da-6cc9a91ca056","cell_type":"code","source":"import pickle\nimport numpy as np\nfrom keras.models import load_model\nfrom keras.preprocessing.sequence import pad_sequences\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output  # Only fatal errors\n\n\n# Load model\nmodel = load_model('/kaggle/input/final-food-rep/seq2seq_model.h5')\n\n# Load tokenizers\nwith open('/kaggle/input/final-food-rep/encoder_tokenizer.pkl', 'rb') as f:\n    encoder_tokenizer = pickle.load(f)\nwith open('/kaggle/input/final-food-rep/decoder_tokenizer.pkl', 'rb') as f:\n    decoder_tokenizer = pickle.load(f)\n\nreverse_decoder_word_index = {i: word for word, i in decoder_tokenizer.word_index.items()}\nstart_token_index = decoder_tokenizer.word_index['start']\nend_token_index = decoder_tokenizer.word_index['end']\n\nmax_encoder_seq_length = model.input[0].shape[1]\nmax_decoder_seq_length = model.output.shape[1]\n\n# --- Decoding function (Greedy or Beam) ---\ndef decode_sequence(input_seq):\n    # Encode the input sequence\n    states_value = encoder_model.predict(input_seq)\n\n    # Initialize the target sequence with the <start> token\n    target_seq = np.array([[decoder_tokenizer.word_index['<start>']]])\n\n\n    stop_condition = False\n    decoded_tokens = []\n    max_target_len = 100  # You can increase this\n\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n\n        # Get the token with the highest probability\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_token = decoder_tokenizer.index_word.get(sampled_token_index, '')\n\n        # Stop if <end> token or max length is reached\n        if sampled_token == '<end>' or len(decoded_tokens) > max_target_len:\n            stop_condition = True\n        else:\n            if len(decoded_tokens) == 0 or sampled_token != decoded_tokens[-1]:  # Prevent repetition\n                decoded_tokens.append(sampled_token)\n\n            # Update target sequence and states\n            target_seq = np.array([[sampled_token_index]])\n            states_value = [h, c]\n            \n\n    return ' '.join(decoded_tokens)\n    \n# --- UI Components ---\ninput_box = widgets.Text(\n    value='onion tomato garlic',\n    placeholder='Enter ingredients...',\n    description='Ingredients:',\n    layout=widgets.Layout(width='600px')\n)\noutput_box = widgets.Output()\nbutton = widgets.Button(description=\"Generate Instruction\", button_style='success')\n\ndef on_button_click(b):\n    output_box.clear_output()\n    input_text = input_box.value.lower()\n    sequence = encoder_tokenizer.texts_to_sequences([input_text])\n    padded_seq = pad_sequences(sequence, maxlen=max_encoder_seq_length, padding='post')\n    \n    with output_box:\n        print(\"Model loaded. Now predicting...\")\n        prediction = decode_sequence(padded_seq)\n        print(f\"Instruction:\\n{prediction}\")\n\nbutton.on_click(on_button_click)\n\n# --- Display UI ---\ndisplay(input_box, button, output_box)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T22:03:36.151491Z","iopub.execute_input":"2025-05-20T22:03:36.151805Z","iopub.status.idle":"2025-05-20T22:03:36.903340Z","shell.execute_reply.started":"2025-05-20T22:03:36.151784Z","shell.execute_reply":"2025-05-20T22:03:36.902336Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Text(value='onion tomato garlic', description='Ingredients:', layout=Layout(width='600px'), placeholder='Enter…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05dc83f282204dd38191a66623f123c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Button(button_style='success', description='Generate Instruction', style=ButtonStyle())","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b83de0b16024664b8de1a7f164a6425"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eee529b0d35c4c6289c8a19d3568d001"}},"metadata":{}}],"execution_count":64},{"id":"b3898a22-847e-48cb-8ba2-86fd5c46e3b8","cell_type":"code","source":"def evaluate_recipe_quality(generated_text):\n    metrics = {}\n    \n    # 1. Coherence Score (0-1)\n    metrics['coherence'] = check_sentence_structure(generated_text)\n    \n    # 2. Repetition Score (lower is better)\n    metrics['repetition'] = calculate_repetition_penalty(generated_text)\n    \n    # 3. Recipe Completeness (0-1)\n    metrics['completeness'] = check_recipe_steps(generated_text)\n    \n    # 4. Indian Context Score (0-1)\n    metrics['indian_context'] = check_indian_cooking_terms(generated_text)\n    \n    return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T22:04:33.757869Z","iopub.execute_input":"2025-05-20T22:04:33.758180Z","iopub.status.idle":"2025-05-20T22:04:33.764564Z","shell.execute_reply.started":"2025-05-20T22:04:33.758160Z","shell.execute_reply":"2025-05-20T22:04:33.763362Z"}},"outputs":[],"execution_count":67},{"id":"e2bf3635-db4d-4317-be21-014ed235bd80","cell_type":"code","source":"# import re\n# import numpy as np\n# from collections import defaultdict\n\n# # Common Indian cooking verbs and connector words to guide generation\n# INDIAN_COOKING_VERBS = [\n#     \"cook\", \"fry\", \"roast\", \"toast\", \"boil\", \"simmer\", \"sauté\", \"temper\", \"garnish\", \"grind\", \n#     \"blend\", \"mix\", \"knead\", \"ferment\", \"steam\", \"pressure cook\", \"stir\", \"add\", \"sprinkle\", \n#     \"pour\", \"marinate\", \"soak\", \"drain\", \"strain\", \"chop\", \"dice\", \"mince\", \"grate\", \"serve\"\n# ]\n\n# CONNECTOR_WORDS = [\n#     \"then\", \"next\", \"after\", \"finally\", \"meanwhile\", \"until\", \"when\", \"and\", \"first\", \"lastly\",\n#     \"once\", \"allow\", \"let\", \"keep\", \"continue\", \"before\", \"while\"\n# ]\n\n# # Common Indian spices and ingredients to boost in vocabulary\n# INDIAN_INGREDIENTS = [\n#     \"cumin\", \"coriander\", \"turmeric\", \"cardamom\", \"cloves\", \"cinnamon\", \"mustard seeds\", \n#     \"fenugreek\", \"asafoetida\", \"curry leaves\", \"garam masala\", \"chaat masala\", \"amchur\", \n#     \"saunf\", \"ajwain\", \"kalonji\", \"panch phoron\", \"ghee\", \"jaggery\", \"tamarind\", \n#     \"coconut\", \"basmati\", \"besan\", \"dal\", \"chana\", \"moong\", \"urad\", \"toor\", \"masoor\", \n#     \"rajma\", \"paneer\", \"yogurt\", \"curd\", \"chilli\", \"ginger\", \"garlic\"\n# ]\n\n# # Regional Indian ingredients to recognize (from your output example)\n# REGIONAL_INDIAN_INGREDIENTS = [\n#     \"badanekayi\", \"gulkand\", \"chekke\", \"amlechi\", \"kewda\", \"kuvar\", \"pulikacha\"\n# ]\n\n# class IndianRecipeGenerator:\n#     def __init__(self, encoder_model, decoder_model, encoder_tokenizer, decoder_tokenizer, \n#                  reverse_decoder_word_index, max_length=100):\n#         self.encoder_model = encoder_model\n#         self.decoder_model = decoder_model\n#         self.encoder_tokenizer = encoder_tokenizer\n#         self.decoder_tokenizer = decoder_tokenizer\n#         self.reverse_decoder_word_index = reverse_decoder_word_index\n#         self.max_length = max_length\n        \n#         # Special tokens if we need to add them to the tokenizer\n#         self.structure_tokens = {\n#             \"start_ingredients\": \"<ingredients>\",\n#             \"end_ingredients\": \"</ingredients>\",\n#             \"start_instructions\": \"<instructions>\",\n#             \"end_instructions\": \"</instructions>\",\n#             \"start_recipe\": \"<start>\",\n#             \"end_recipe\": \"<end>\"\n#         }\n    \n#     def clean_text(self, text):\n#         \"\"\"Clean the generated text by removing non-alphanumeric characters except spaces.\"\"\"\n#         # Remove non-alphanumeric characters except spaces and basic punctuation\n#         text = re.sub(r'[^\\w\\s.,;:!?-]', '', text)\n#         # Remove extra spaces\n#         text = re.sub(r'\\s+', ' ', text)\n#         return text.strip()\n    \n#     def has_proper_sentence_structure(self, text):\n#         \"\"\"Check if the text has proper sentence structure with cooking verbs.\"\"\"\n#         # Split text into sentences\n#         sentences = re.split(r'[.!?]+', text)\n#         valid_sentences = 0\n        \n#         for sentence in sentences:\n#             sentence = sentence.strip().lower()\n#             if not sentence:\n#                 continue\n                \n#             # Check if sentence contains a cooking verb\n#             has_verb = any(verb in sentence for verb in INDIAN_COOKING_VERBS)\n            \n#             # Check if sentence has some basic structure (subject + verb pattern)\n#             # This is a simplified check - in reality you'd want more sophisticated NLP\n#             if has_verb and len(sentence.split()) >= 3:\n#                 valid_sentences += 1\n        \n#         # Return True if at least 70% of sentences are valid\n#         return valid_sentences >= max(1, len(sentences) * 0.7)\n    \n#     def boost_token_logits(self, logits, current_context=None, instruction_mode=True):\n#         \"\"\"Boost logits for appropriate tokens based on context.\"\"\"\n#         # Get vocabulary as list of words\n#         vocab = list(self.reverse_decoder_word_index.values())\n        \n#         # Determine which kind of tokens to boost based on context\n#         boost_tokens = []\n#         penalty_tokens = []\n        \n#         if instruction_mode:\n#             # In instruction mode, boost cooking verbs and connectors\n#             boost_tokens = INDIAN_COOKING_VERBS + CONNECTOR_WORDS\n            \n#             # Also boost ingredients but with lower priority\n#             boost_tokens += INDIAN_INGREDIENTS + REGIONAL_INDIAN_INGREDIENTS\n#         else:\n#             # In ingredient mode, boost ingredients\n#             boost_tokens = INDIAN_INGREDIENTS + REGIONAL_INDIAN_INGREDIENTS\n            \n#         # Apply boosts and penalties\n#         for i, word in enumerate(vocab):\n#             # Apply boost to relevant tokens\n#             if any(token in word.lower() for token in boost_tokens):\n#                 logits[i] *= 1.2  # 20% boost\n                \n#             # Penalize repetition if we have context\n#             if current_context and word in current_context.split()[-5:]:\n#                 logits[i] *= 0.5  # 50% penalty for recently used words\n                \n#         return logits\n    \n#     def top_p_filtering(self, logits, top_p=0.92):\n#         \"\"\"Filter logits using nucleus (top-p) sampling.\"\"\"\n#         # Sort logits in descending order\n#         sorted_indices = np.argsort(logits)[::-1]\n#         sorted_logits = logits[sorted_indices]\n        \n#         # Calculate cumulative probabilities\n#         sorted_probs = np.exp(sorted_logits) / np.sum(np.exp(sorted_logits))\n#         cumulative_probs = np.cumsum(sorted_probs)\n        \n#         # Remove tokens with cumulative probability above threshold\n#         sorted_indices_to_remove = cumulative_probs > top_p\n#         # Shift indices to keep first token above threshold\n#         sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1]\n#         sorted_indices_to_remove[0] = False\n        \n#         # Create a mask for indices to remove\n#         indices_to_remove = np.zeros_like(logits, dtype=bool)\n#         indices_to_remove[sorted_indices[sorted_indices_to_remove]] = True\n        \n#         # Set filtered logits to negative infinity\n#         filtered_logits = logits.copy()\n#         filtered_logits[indices_to_remove] = -float('inf')\n        \n#         return filtered_logits\n    \n#     def generate_recipe(self, input_text, beam_width=5, temperature=1.0, \n#                         top_p=0.92, length_penalty=0.7):\n#         \"\"\"Generate a structured Indian recipe from input text.\"\"\"\n#         # Tokenize input\n#         input_seq = self.encoder_tokenizer.texts_to_sequences([input_text])\n#         input_seq = np.array(input_seq)\n        \n#         # Encode input\n#         states_value = self.encoder_model.predict(input_seq)\n        \n#         # Get start and end token indices\n#         start_token_index = self.decoder_tokenizer.word_index.get('start', 1)\n#         end_token_index = self.decoder_tokenizer.word_index.get('end', 2)\n        \n#         # Initialize beam search\n#         sequences = [([start_token_index], 0.0, states_value, '')]\n        \n#         # Track all generated n-grams to prevent repetition\n#         all_ngrams = defaultdict(int)\n        \n#         # Flag to track if we're in ingredients or instructions section\n#         instruction_mode = False\n        \n#         # Generate sequence with beam search\n#         for _ in range(self.max_length):\n#             all_candidates = []\n            \n#             for seq, score, state, text_so_far in sequences:\n#                 if seq[-1] == end_token_index:\n#                     all_candidates.append((seq, score, state, text_so_far))\n#                     continue\n                \n#                 # Get last token as input to decoder\n#                 target_seq = np.array([[seq[-1]]])\n                \n#                 # Predict next token\n#                 output_tokens, h, c = self.decoder_model.predict([target_seq] + state)\n#                 logits = output_tokens[0, -1, :]\n                \n#                 # Apply temperature\n#                 logits = logits / temperature\n                \n#                 # Switch modes based on special tokens\n#                 # This assumes you've added structure tokens to your vocabulary\n#                 if \"</ingredients>\" in text_so_far and not instruction_mode:\n#                     instruction_mode = True\n                \n#                 # Boost appropriate tokens based on mode\n#                 logits = self.boost_token_logits(logits, text_so_far, instruction_mode)\n                \n#                 # Apply top-p filtering\n#                 logits = self.top_p_filtering(logits, top_p)\n                \n#                 # Get top candidates\n#                 top_indices = np.argsort(logits)[-beam_width*2:]\n                \n#                 for idx in top_indices:\n#                     if logits[idx] == -float('inf'):\n#                         continue  # Skip tokens filtered by top-p\n                        \n#                     # Calculate probability and log probability\n#                     token_prob = np.exp(logits[idx]) / np.sum(np.exp(logits[np.isfinite(logits)]))\n#                     token_log_prob = np.log(token_prob + 1e-9)\n                    \n#                     # Create new candidate\n#                     new_seq = seq + [idx]\n#                     new_score = score - token_log_prob\n#                     new_text = text_so_far\n                    \n#                     # Convert token to word and add to text\n#                     word = self.reverse_decoder_word_index.get(idx, '')\n#                     if word and word not in ['start', 'end']:\n#                         if new_text and new_text[-1] not in [' ', '\\n']:\n#                             new_text += ' '\n#                         new_text += word\n                    \n#                     # Check for repetitions\n#                     repeat_penalty = 0\n                    \n#                     # Check for token repetition\n#                     if len(new_seq) > 2 and new_seq[-1] == new_seq[-2]:\n#                         repeat_penalty += 2.0\n                    \n#                     # Check for n-gram repetitions\n#                     for n in range(2, 5):\n#                         if len(new_seq) >= n:\n#                             ngram = tuple(new_seq[-n:])\n#                             all_ngrams[ngram] += 1\n#                             if all_ngrams[ngram] > 1:\n#                                 repeat_penalty += 1.0 * all_ngrams[ngram]\n                    \n#                     # Apply repeat penalty\n#                     new_score += repeat_penalty\n                    \n#                     # Add candidate\n#                     all_candidates.append((new_seq, new_score, [h, c], new_text))\n            \n#             # Apply length normalization\n#             normalized_candidates = []\n#             for seq, score, state, text in all_candidates:\n#                 # Normalize score by length\n#                 norm_score = score / (len(seq) ** length_penalty)\n#                 normalized_candidates.append((seq, norm_score, state, text))\n            \n#             # Sort and select top beam_width candidates\n#             sequences = sorted(normalized_candidates, key=lambda x: x[1])[:beam_width]\n            \n#             # Early stopping if all sequences ended\n#             if all(seq[-1] == end_token_index for seq, _, _, _ in sequences):\n#                 break\n        \n#         # Get best sequence\n#         best_seq, _, _, best_text = sequences[0]\n        \n#         # Post-process the text\n#         recipe = self.post_process_recipe(best_text)\n        \n#         return recipe\n    \n#     def post_process_recipe(self, text):\n#         \"\"\"Apply post-processing to the generated recipe text.\"\"\"\n#         # Remove special tokens if present\n#         for token in self.structure_tokens.values():\n#             text = text.replace(token, '')\n        \n#         # Clean up the text\n#         text = self.clean_text(text)\n        \n#         # Split into sections based on sentence structure\n#         sentences = re.split(r'([.!?] )', text)\n#         processed_text = \"\"\n        \n#         # Reassemble text with proper sentence structure\n#         for i in range(0, len(sentences) - 1, 2):\n#             sentence = sentences[i]\n#             ending = sentences[i + 1] if i + 1 < len(sentences) else \". \"\n            \n#             # Fix common issues\n#             # 1. Ensure sentence starts with capital letter\n#             if sentence and not sentence[0].isupper():\n#                 sentence = sentence[0].upper() + sentence[1:]\n            \n#             # 2. Make sure cooking verbs are used properly\n#             has_verb = any(verb in sentence.lower() for verb in INDIAN_COOKING_VERBS)\n#             if not has_verb and len(sentence.split()) > 3:\n#                 # Try to inject a cooking verb if missing\n#                 words = sentence.split()\n#                 for i, word in enumerate(words):\n#                     if word.lower() in INDIAN_INGREDIENTS or word.lower() in REGIONAL_INDIAN_INGREDIENTS:\n#                         # Add a cooking verb after an ingredient\n#                         verb_index = min(i + 1, len(words))\n#                         words.insert(verb_index, \"cook\")\n#                         break\n#                 sentence = \" \".join(words)\n            \n#             processed_text += sentence + ending\n        \n#         # Add the last sentence if there's an odd number\n#         if len(sentences) % 2 == 1:\n#             last_sentence = sentences[-1]\n#             if last_sentence and not last_sentence[0].isupper():\n#                 last_sentence = last_sentence[0].upper() + last_sentence[1:]\n#             processed_text += last_sentence\n            \n#             # Make sure it ends with a period\n#             if not processed_text.endswith(('.', '!', '?')):\n#                 processed_text += '.'\n        \n#         # Structure the recipe into steps\n#         recipe_lines = processed_text.strip().split('. ')\n#         structured_recipe = \"\"\n        \n#         for i, line in enumerate(recipe_lines):\n#             if line:\n#                 # Add step numbers\n#                 structured_recipe += f\"{i+1}. {line}\"\n#                 # Add period if missing\n#                 if not structured_recipe.endswith(('.', '!', '?')):\n#                     structured_recipe += '.'\n#                 structured_recipe += '\\n'\n        \n#         return structured_recipe.strip()\n    \n#     def format_with_structure_tokens(self, recipe_text):\n#         \"\"\"Add structure tokens to the recipe if not present.\"\"\"\n#         if \"<ingredients>\" not in recipe_text:\n#             # Try to identify ingredients and instructions sections\n#             lines = recipe_text.strip().split('\\n')\n#             ingredients_section = []\n#             instructions_section = []\n            \n#             in_ingredients = True\n            \n#             for line in lines:\n#                 if line.strip():\n#                     if (any(x in line.lower() for x in ['step', 'instruction', 'method', 'procedure']) or \n#                         re.match(r'^\\d+\\.', line)):\n#                         in_ingredients = False\n                    \n#                     if in_ingredients:\n#                         ingredients_section.append(line)\n#                     else:\n#                         instructions_section.append(line)\n            \n#             # Format with structure tokens\n#             formatted_recipe = \"<ingredients>\\n\"\n#             formatted_recipe += '\\n'.join(ingredients_section)\n#             formatted_recipe += \"\\n</ingredients>\\n\\n<instructions>\\n\"\n#             formatted_recipe += '\\n'.join(instructions_section)\n#             formatted_recipe += \"\\n</instructions>\"\n            \n#             return formatted_recipe\n#         else:\n#             return recipe_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T22:03:36.904894Z","iopub.execute_input":"2025-05-20T22:03:36.905287Z","iopub.status.idle":"2025-05-20T22:03:36.920919Z","shell.execute_reply.started":"2025-05-20T22:03:36.905265Z","shell.execute_reply":"2025-05-20T22:03:36.919943Z"}},"outputs":[],"execution_count":65},{"id":"ce37bf63-d6fb-465f-8b37-f9c178969456","cell_type":"code","source":"# import pickle\n# import numpy as np\n# from keras.models import load_model\n# from keras.preprocessing.sequence import pad_sequences\n# import ipywidgets as widgets\n# from IPython.display import display, clear_output\n\n# # Load model\n# model = load_model('/kaggle/input/final-food-rep/seq2seq_model.h5')\n\n# # Load tokenizers\n# with open('/kaggle/input/final-food-rep/encoder_tokenizer.pkl', 'rb') as f:\n#     encoder_tokenizer = pickle.load(f)\n# with open('/kaggle/input/final-food-rep/decoder_tokenizer.pkl', 'rb') as f:\n#     decoder_tokenizer = pickle.load(f)\n\n# reverse_decoder_word_index = {i: word for word, i in decoder_tokenizer.word_index.items()}\n# start_token_index = decoder_tokenizer.word_index['start']\n# end_token_index = decoder_tokenizer.word_index['end']\n\n# max_encoder_seq_length = model.input[0].shape[1]\n# max_decoder_seq_length = model.output.shape[1]\n\n# # # Generate a recipe\n# # recipe = generator.generate_recipe(\"chicken onion tomato butter\")\n# # print(recipe)\n\n# # import os\n# # os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Only fatal errors\n\n\n# generator = IndianRecipeGenerator(\n#     encoder_model, \n#     decoder_model, \n#     encoder_tokenizer, \n#     decoder_tokenizer, \n#     reverse_decoder_word_index\n# )\n\n# # --- UI Components ---\n# input_box = widgets.Text(\n#     value='onion tomato garlic',\n#     placeholder='Enter ingredients...',\n#     description='Ingredients:',\n#     layout=widgets.Layout(width='600px')\n# )\n# output_box = widgets.Output()\n# button = widgets.Button(description=\"Generate Instruction\", button_style='success')\n\n# def on_button_click(b):\n#     output_box.clear_output()\n#     input_text = input_box.value.lower()\n#     sequence = encoder_tokenizer.texts_to_sequences([input_text])\n#     padded_seq = pad_sequences(sequence, maxlen=max_encoder_seq_length, padding='post')\n    \n#     with output_box:\n#         prediction = generate_recipe(padded_seq)\n#         print(f\"Instruction:\\n{prediction}\")\n\n# button.on_click(on_button_click)\n\n# # --- Display UI ---\n# display(input_box, button, output_box)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T22:03:36.921950Z","iopub.execute_input":"2025-05-20T22:03:36.922270Z","iopub.status.idle":"2025-05-20T22:03:36.946650Z","shell.execute_reply.started":"2025-05-20T22:03:36.922240Z","shell.execute_reply":"2025-05-20T22:03:36.945637Z"}},"outputs":[],"execution_count":66},{"id":"446b98c9-9e95-46c2-8c7d-31d3f177e478","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}