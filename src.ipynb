{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e939283",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T08:38:36.706939Z",
     "iopub.status.busy": "2025-05-18T08:38:36.706408Z",
     "iopub.status.idle": "2025-05-18T08:38:41.158392Z",
     "shell.execute_reply": "2025-05-18T08:38:41.157298Z",
     "shell.execute_reply.started": "2025-05-18T08:38:36.706915Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\python312\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\python312\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (2.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\python312\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.25.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (71.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.65.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.0)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.7.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy pandas scikit-learn tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc89318b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T08:38:41.160759Z",
     "iopub.status.busy": "2025-05-18T08:38:41.160080Z",
     "iopub.status.idle": "2025-05-18T08:38:56.350399Z",
     "shell.execute_reply": "2025-05-18T08:38:56.349752Z",
     "shell.execute_reply.started": "2025-05-18T08:38:41.160730Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6450b5c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T08:38:56.351691Z",
     "iopub.status.busy": "2025-05-18T08:38:56.351206Z",
     "iopub.status.idle": "2025-05-18T08:38:56.687811Z",
     "shell.execute_reply": "2025-05-18T08:38:56.686870Z",
     "shell.execute_reply.started": "2025-05-18T08:38:56.351671Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Cleaned_Indian_Food_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc8d1083",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T08:38:56.690190Z",
     "iopub.status.busy": "2025-05-18T08:38:56.689871Z",
     "iopub.status.idle": "2025-05-18T08:38:56.694800Z",
     "shell.execute_reply": "2025-05-18T08:38:56.693969Z",
     "shell.execute_reply.started": "2025-05-18T08:38:56.690169Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing function (optional but recommended)\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower().strip()       # Ensure string, lowercase, and strip\n",
    "    text = ' '.join(text.split())          # Remove extra whitespace\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22464759-9c30-4009-831d-1b8dc0fef6c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T08:38:56.695949Z",
     "iopub.status.busy": "2025-05-18T08:38:56.695645Z",
     "iopub.status.idle": "2025-05-18T08:38:56.856571Z",
     "shell.execute_reply": "2025-05-18T08:38:56.855882Z",
     "shell.execute_reply.started": "2025-05-18T08:38:56.695923Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "input_texts = [preprocess_text(text) for text in df[\"Cleaned-Ingredients\"]]\n",
    "target_texts = [\"<start> \" + preprocess_text(text) + \" <end>\" for text in df[\"TranslatedInstructions\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "934852c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T08:38:56.857981Z",
     "iopub.status.busy": "2025-05-18T08:38:56.857743Z",
     "iopub.status.idle": "2025-05-18T08:38:57.092156Z",
     "shell.execute_reply": "2025-05-18T08:38:57.091162Z",
     "shell.execute_reply.started": "2025-05-18T08:38:56.857959Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "encoder_tokenizer = Tokenizer()\n",
    "encoder_tokenizer.fit_on_texts(input_texts)\n",
    "encoder_sequences = encoder_tokenizer.texts_to_sequences(input_texts)\n",
    "encoder_input_data = pad_sequences(encoder_sequences, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad4e2af2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T09:57:26.662976Z",
     "iopub.status.busy": "2025-05-18T09:57:26.662309Z",
     "iopub.status.idle": "2025-05-18T09:57:27.943399Z",
     "shell.execute_reply": "2025-05-18T09:57:27.942829Z",
     "shell.execute_reply.started": "2025-05-18T09:57:26.662955Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "decoder_tokenizer = Tokenizer(filters='')\n",
    "decoder_tokenizer.fit_on_texts(target_texts)\n",
    "reverse_decoder_word_index = {index: word for word, index in decoder_tokenizer.word_index.items()}\n",
    "decoder_sequences = decoder_tokenizer.texts_to_sequences(target_texts)\n",
    "decoder_input_data = pad_sequences([seq[:-1] for seq in decoder_sequences], padding='post')\n",
    "decoder_target_data = pad_sequences([seq[1:] for seq in decoder_sequences], padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae120001-56eb-422e-a2bb-9cf93a7173ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T09:57:29.678431Z",
     "iopub.status.busy": "2025-05-18T09:57:29.678161Z",
     "iopub.status.idle": "2025-05-18T09:57:29.707643Z",
     "shell.execute_reply": "2025-05-18T09:57:29.706914Z",
     "shell.execute_reply.started": "2025-05-18T09:57:29.678412Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load encoder tokenizer\n",
    "with open('encoder_tokenizer.pkl', 'rb') as f:\n",
    "    encoder_tokenizer = pickle.load(f)\n",
    "\n",
    "# Load decoder tokenizer\n",
    "with open('decoder_tokenizer.pkl', 'rb') as f:\n",
    "    decoder_tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec6a2354-4a35-4421-9af7-8fc5c97d959d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T09:57:34.832375Z",
     "iopub.status.busy": "2025-05-18T09:57:34.832081Z",
     "iopub.status.idle": "2025-05-18T09:57:34.836406Z",
     "shell.execute_reply": "2025-05-18T09:57:34.835678Z",
     "shell.execute_reply.started": "2025-05-18T09:57:34.832355Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "encoder_vocab_size = len(encoder_tokenizer.word_index) + 1\n",
    "decoder_vocab_size = len(decoder_tokenizer.word_index) + 1\n",
    "embedding_dim = 128\n",
    "latent_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25eaeeb0-b3bb-4a84-84af-e8d102a5d107",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T09:57:36.161753Z",
     "iopub.status.busy": "2025-05-18T09:57:36.161490Z",
     "iopub.status.idle": "2025-05-18T09:57:36.219389Z",
     "shell.execute_reply": "2025-05-18T09:57:36.218730Z",
     "shell.execute_reply.started": "2025-05-18T09:57:36.161733Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb = Embedding(encoder_vocab_size, embedding_dim)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "_, state_h, state_c = encoder_lstm(enc_emb)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab8a4ac8-1683-4cef-8c4c-c5636eb04480",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T09:57:38.029338Z",
     "iopub.status.busy": "2025-05-18T09:57:38.028642Z",
     "iopub.status.idle": "2025-05-18T09:57:38.080142Z",
     "shell.execute_reply": "2025-05-18T09:57:38.079399Z",
     "shell.execute_reply.started": "2025-05-18T09:57:38.029316Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(decoder_vocab_size, embedding_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "decoder_dense = Dense(decoder_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fb13baf-d98d-4112-8b61-68e1a6ad0021",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T08:39:01.512306Z",
     "iopub.status.busy": "2025-05-18T08:39:01.511960Z",
     "iopub.status.idle": "2025-05-18T08:39:01.546988Z",
     "shell.execute_reply": "2025-05-18T08:39:01.546079Z",
     "shell.execute_reply.started": "2025-05-18T08:39:01.512280Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">307,840</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,276,352</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],       │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]        │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">8,586,113</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">33409</span>)            │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │    \u001b[38;5;34m307,840\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │  \u001b[38;5;34m4,276,352\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),     │    \u001b[38;5;34m394,240\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │    \u001b[38;5;34m394,240\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],       │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]        │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │  \u001b[38;5;34m8,586,113\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│                     │ \u001b[38;5;34m33409\u001b[0m)            │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,958,785</span> (53.25 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m13,958,785\u001b[0m (53.25 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,958,785</span> (53.25 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m13,958,785\u001b[0m (53.25 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08a640a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T08:39:01.548206Z",
     "iopub.status.busy": "2025-05-18T08:39:01.547872Z",
     "iopub.status.idle": "2025-05-18T08:39:01.553352Z",
     "shell.execute_reply": "2025-05-18T08:39:01.552652Z",
     "shell.execute_reply.started": "2025-05-18T08:39:01.548181Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# checkpoint = ModelCheckpoint(\n",
    "#     filepath='/kaggle/working/model_check.weights.h5',  # or .h5 or .weights.h5\n",
    "#     save_weights_only=True,\n",
    "#     save_best_only=False,\n",
    "#     save_freq='epoch',\n",
    "#     verbose=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a78413c5-f48f-4fda-b057-7ad17ff3a0dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T08:39:01.554781Z",
     "iopub.status.busy": "2025-05-18T08:39:01.554460Z",
     "iopub.status.idle": "2025-05-18T08:39:01.568694Z",
     "shell.execute_reply": "2025-05-18T08:39:01.567977Z",
     "shell.execute_reply.started": "2025-05-18T08:39:01.554755Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Remove all extra dimensions\n",
    "# decoder_target_data = np.squeeze(decoder_target_data)\n",
    "\n",
    "# # Add just one last dimension\n",
    "# decoder_target_data = np.expand_dims(decoder_target_data, -1)\n",
    "\n",
    "# # Confirm final shape\n",
    "# print(decoder_target_data.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e79407ed-c435-4966-a385-c7c7e3144d5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T08:39:26.591579Z",
     "iopub.status.busy": "2025-05-18T08:39:26.590648Z",
     "iopub.status.idle": "2025-05-18T09:24:58.115989Z",
     "shell.execute_reply": "2025-05-18T09:24:58.115274Z",
     "shell.execute_reply.started": "2025-05-18T08:39:26.591554Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from keras.callbacks import EarlyStopping\n",
    "\n",
    "# early_stopping = EarlyStopping(\n",
    "#     monitor='val_loss',\n",
    "#     patience=3,  # stops if val_loss doesn't improve for 3 consecutive epochs\n",
    "#     restore_best_weights=True\n",
    "# )\n",
    "\n",
    "# model.fit(\n",
    "#     [encoder_input_data, decoder_input_data],\n",
    "#     decoder_target_data,\n",
    "#     batch_size=16,\n",
    "#     epochs=50,\n",
    "#     validation_split=0.2,\n",
    "#     callbacks=[early_stopping]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aaab62b3-4663-4fe1-86d9-2dbfc18dee70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T09:25:20.210825Z",
     "iopub.status.busy": "2025-05-18T09:25:20.210507Z",
     "iopub.status.idle": "2025-05-18T09:25:20.546456Z",
     "shell.execute_reply": "2025-05-18T09:25:20.545707Z",
     "shell.execute_reply.started": "2025-05-18T09:25:20.210801Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "# model.save('/kaggle/working/seq2seq_model.h5')\n",
    "model= load_model(\"seq2seq_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f65c31ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def clean_output(text):\n",
    "#     # Remove non-alphabetic characters except spaces\n",
    "#     text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "#     # Remove extra spaces\n",
    "#     text = re.sub(r'\\s+', ' ', text)\n",
    "#     return text.strip()\n",
    "\n",
    "# def decode_sequence_beam_search(input_seq, beam_width=5, max_decoder_seq_length=2500):\n",
    "#     states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "#     start_token_index = decoder_tokenizer.word_index.get('start')\n",
    "#     end_token_index = decoder_tokenizer.word_index.get('end')\n",
    "    \n",
    "#     sequences = [([start_token_index], 0.0, states_value)]\n",
    "\n",
    "#     for _ in range(max_decoder_seq_length):\n",
    "#         all_candidates = []\n",
    "#         for seq, score, state in sequences:\n",
    "#             if seq[-1] == end_token_index:\n",
    "#                 all_candidates.append((seq, score, state))\n",
    "#                 continue\n",
    "            \n",
    "#             target_seq = np.array([[seq[-1]]])\n",
    "#             output_tokens, h, c = decoder_model.predict([target_seq] + state)\n",
    "\n",
    "#             top_k = np.argsort(output_tokens[0, -1, :])[-beam_width:]\n",
    "\n",
    "#             for word_id in top_k:\n",
    "#                 word_prob = output_tokens[0, -1, word_id]\n",
    "#                 candidate_seq = seq + [word_id]\n",
    "#                 candidate_score = score - np.log(word_prob + 1e-9)\n",
    "\n",
    "#                 # Penalize repetition of last token\n",
    "#                 if len(candidate_seq) > 2 and candidate_seq[-1] == candidate_seq[-2]:\n",
    "#                     candidate_score += 1.0  # Increase penalty as needed\n",
    "\n",
    "#                 # Penalize n-gram repetition\n",
    "#                 if has_repeat_ngram(candidate_seq, n=3, window_size=12):\n",
    "#                     candidate_score += 2.0  # Increase penalty as needed\n",
    "\n",
    "#                 all_candidates.append((candidate_seq, candidate_score, [h, c]))\n",
    "        \n",
    "#         ordered = sorted(all_candidates, key=lambda tup: tup[1])\n",
    "#         sequences = ordered[:beam_width]\n",
    "\n",
    "#         # Early stopping if all sequences ended\n",
    "#         if all(seq[-1] == end_token_index for seq, _, _ in sequences):\n",
    "#             break\n",
    "\n",
    "#     best_seq = sequences[0][0]\n",
    "\n",
    "#     # Convert to words, filter out unwanted tokens\n",
    "#     decoded_tokens = [\n",
    "#         reverse_decoder_word_index.get(idx, '') \n",
    "#         for idx in best_seq \n",
    "#         if idx not in [start_token_index, end_token_index] and \n",
    "#            reverse_decoder_word_index.get(idx, '').isalpha()\n",
    "#     ]\n",
    "#     decoded_sentence = ' '.join(decoded_tokens)\n",
    "#     decoded_sentence = remove_duplicate_phrases(decoded_sentence.strip())\n",
    "#     decoded_sentence = clean_output(decoded_sentence)\n",
    "#     return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3015a93-1557-4845-b269-12fc8a0b20a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T09:26:03.479335Z",
     "iopub.status.busy": "2025-05-18T09:26:03.478683Z",
     "iopub.status.idle": "2025-05-18T09:26:03.485088Z",
     "shell.execute_reply": "2025-05-18T09:26:03.484100Z",
     "shell.execute_reply.started": "2025-05-18T09:26:03.479308Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Encoder inference model\n",
    "encoder_model = Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7549616-e387-451c-8b24-ff32153b55fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T09:26:06.148689Z",
     "iopub.status.busy": "2025-05-18T09:26:06.148386Z",
     "iopub.status.idle": "2025-05-18T09:26:06.158873Z",
     "shell.execute_reply": "2025-05-18T09:26:06.158057Z",
     "shell.execute_reply.started": "2025-05-18T09:26:06.148667Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Decoder inference model\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "200681a2-8437-4fa1-8999-83c7a365bd68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T09:57:56.426408Z",
     "iopub.status.busy": "2025-05-18T09:57:56.425873Z",
     "iopub.status.idle": "2025-05-18T09:57:56.431889Z",
     "shell.execute_reply": "2025-05-18T09:57:56.431201Z",
     "shell.execute_reply.started": "2025-05-18T09:57:56.426385Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input sequence\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Initialize the target sequence with the <start> token\n",
    "    target_seq = np.array([[decoder_tokenizer.word_index['<start>']]])\n",
    "\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_tokens = []\n",
    "    max_target_len = 100  # You can increase this\n",
    "\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Get the token with the highest probability\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = decoder_tokenizer.index_word.get(sampled_token_index, '')\n",
    "\n",
    "        # Stop if <end> token or max length is reached\n",
    "        if sampled_token == '<end>' or len(decoded_tokens) > max_target_len:\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            if len(decoded_tokens) == 0 or sampled_token != decoded_tokens[-1]:  # Prevent repetition\n",
    "                decoded_tokens.append(sampled_token)\n",
    "\n",
    "            # Update target sequence and states\n",
    "            target_seq = np.array([[sampled_token_index]])\n",
    "            states_value = [h, c]\n",
    "            \n",
    "\n",
    "    return ' '.join(decoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bf3635-db4d-4317-be21-014ed235bd80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T10:10:34.430799Z",
     "iopub.status.busy": "2025-05-18T10:10:34.430019Z",
     "iopub.status.idle": "2025-05-18T10:10:34.440547Z",
     "shell.execute_reply": "2025-05-18T10:10:34.439972Z",
     "shell.execute_reply.started": "2025-05-18T10:10:34.430774Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_output(text):\n",
    "    # Remove non-alphabetic characters except spaces\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def has_repeat_ngram(seq, n=3, window_size=12):\n",
    "    \"\"\"\n",
    "    Returns True if any n-gram of length n repeats within the last window_size tokens of seq.\n",
    "    \"\"\"\n",
    "    if len(seq) < n * 2:\n",
    "        return False\n",
    "    window = seq[-window_size:] if len(seq) > window_size else seq\n",
    "    ngrams = set()\n",
    "    for i in range(len(window) - n + 1):\n",
    "        ngram = tuple(window[i:i+n])\n",
    "        if ngram in ngrams:\n",
    "            return True\n",
    "        ngrams.add(ngram)\n",
    "    return False\n",
    "\n",
    "def decode_sequence_beam_search(input_seq, beam_width=5, max_decoder_seq_length=100):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    start_token_index = decoder_tokenizer.word_index.get('start')\n",
    "    end_token_index = decoder_tokenizer.word_index.get('end')\n",
    "    \n",
    "    sequences = [([start_token_index], 0.0, states_value)]\n",
    "\n",
    "    for _ in range(max_decoder_seq_length):\n",
    "        all_candidates = []\n",
    "        for seq, score, state in sequences:\n",
    "            if seq[-1] == end_token_index:\n",
    "                all_candidates.append((seq, score, state))\n",
    "                continue\n",
    "            \n",
    "            target_seq = np.array([[seq[-1]]])\n",
    "            output_tokens, h, c = decoder_model.predict([target_seq] + state)\n",
    "\n",
    "            top_k = np.argsort(output_tokens[0, -1, :])[-beam_width:]\n",
    "\n",
    "            for word_id in top_k:\n",
    "                word_prob = output_tokens[0, -1, word_id]\n",
    "                candidate_seq = seq + [word_id]\n",
    "                candidate_score = score - np.log(word_prob + 1e-9)\n",
    "\n",
    "                # Penalize repetition of last token\n",
    "                if len(candidate_seq) > 2 and candidate_seq[-1] == candidate_seq[-2]:\n",
    "                    candidate_score += 1.0  # Increase penalty as needed\n",
    "\n",
    "                # Penalize n-gram repetition\n",
    "                if has_repeat_ngram(candidate_seq, n=3, window_size=12):\n",
    "                    candidate_score += 2.0  # Increase penalty as needed\n",
    "\n",
    "                all_candidates.append((candidate_seq, candidate_score, [h, c]))\n",
    "        \n",
    "        ordered = sorted(all_candidates, key=lambda tup: tup[1])\n",
    "        sequences = ordered[:beam_width]\n",
    "\n",
    "        # Early stopping if all sequences ended\n",
    "        if all(seq[-1] == end_token_index for seq, _, _ in sequences):\n",
    "            break\n",
    "\n",
    "    best_seq = sequences[0][0]\n",
    "\n",
    "    # Convert to words, filter out unwanted tokens\n",
    "    decoded_tokens = [\n",
    "        reverse_decoder_word_index.get(idx, '') \n",
    "        for idx in best_seq \n",
    "        if idx not in [start_token_index, end_token_index] and \n",
    "           reverse_decoder_word_index.get(idx, '').isalpha()\n",
    "    ]\n",
    "    decoded_sentence = ' '.join(decoded_tokens)\n",
    "    \n",
    "    def remove_duplicate_phrases(text, min_phrase_len=3):\n",
    "        words = text.split()\n",
    "        result = []\n",
    "        i = 0\n",
    "        while i < len(words):\n",
    "            # Try to find the longest duplicate phrase starting at i\n",
    "            found_duplicate = False\n",
    "            for l in range(min_phrase_len, len(words) - i):\n",
    "                phrase = words[i:i+l]\n",
    "                rest = words[i+l:]\n",
    "                if phrase and rest[:l] == phrase:\n",
    "                    found_duplicate = True\n",
    "                    i += l  # skip the duplicate\n",
    "                    break\n",
    "            if not found_duplicate:\n",
    "                result.append(words[i])\n",
    "                i += 1\n",
    "        return ' '.join(result)\n",
    "\n",
    "    decoded_sentence = remove_duplicate_phrases(decoded_sentence.strip())\n",
    "    decoded_sentence = clean_output(decoded_sentence)\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce37bf63-d6fb-465f-8b37-f9c178969456",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T10:10:35.842688Z",
     "iopub.status.busy": "2025-05-18T10:10:35.842141Z",
     "iopub.status.idle": "2025-05-18T10:10:35.846474Z",
     "shell.execute_reply": "2025-05-18T10:10:35.845808Z",
     "shell.execute_reply.started": "2025-05-18T10:10:35.842664Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict_instruction(input_text, beam_width=5):\n",
    "    seq = encoder_tokenizer.texts_to_sequences([input_text])\n",
    "    padded = pad_sequences(seq, maxlen=encoder_input_data.shape[1], padding='post')\n",
    "        \n",
    "    return decode_sequence_beam_search(padded, beam_width=beam_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709de312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import random\n",
    "# from collections import defaultdict\n",
    "\n",
    "# class RecipeTextReconstructor:\n",
    "#     \"\"\"\n",
    "#     A rule-based system to transform broken recipe outputs into coherent instructions.\n",
    "#     This is a stopgap solution when the underlying model produces severely damaged text.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         # Common Indian cooking ingredients\n",
    "#         self.indian_ingredients = [\n",
    "#             \"ghee\", \"cumin\", \"coriander\", \"turmeric\", \"garam masala\", \"cardamom\", \n",
    "#             \"cloves\", \"cinnamon\", \"mustard seeds\", \"fenugreek\", \"asafoetida\", \"curry leaves\",\n",
    "#             \"chaat masala\", \"amchur\", \"panch phoran\", \"jaggery\", \"tamarind\", \"coconut\",\n",
    "#             \"basmati rice\", \"besan\", \"dal\", \"chana\", \"moong\", \"urad\", \"toor\", \"masoor\",\n",
    "#             \"rajma\", \"paneer\", \"yogurt\", \"curd\", \"chilli\", \"ginger\", \"garlic\", \"onion\",\n",
    "#             \"tomato\", \"potato\", \"okra\", \"eggplant\", \"cauliflower\", \"peas\"\n",
    "#         ]\n",
    "        \n",
    "#         # Common Indian dishes by region\n",
    "#         self.indian_dishes = {\n",
    "#             \"North Indian\": [\"butter chicken\", \"dal makhani\", \"paneer tikka\", \"chole bhature\", \n",
    "#                          \"rajma chawal\", \"kadhi pakora\", \"aloo gobi\", \"malai kofta\"],\n",
    "#             \"South Indian\": [\"dosa\", \"idli\", \"sambhar\", \"rasam\", \"uttapam\", \"vada\", \n",
    "#                          \"appam\", \"pongal\", \"bisi bele bath\"],\n",
    "#             \"Bengali\": [\"machher jhol\", \"shorshe ilish\", \"chingri malai curry\", \n",
    "#                      \"kosha mangsho\", \"aloo posto\", \"mishti doi\"],\n",
    "#             \"Gujarati\": [\"dhokla\", \"khandvi\", \"thepla\", \"undhiyu\", \"fafda\", \"khakhra\", \"gathiya\"],\n",
    "#             \"Maharashtrian\": [\"vada pav\", \"pav bhaji\", \"misal pav\", \"puran poli\", \"bharli vangi\"],\n",
    "#             \"Hyderabadi\": [\"biryani\", \"haleem\", \"keema\", \"mirchi ka salan\", \"bagara baingan\"]\n",
    "#         }\n",
    "        \n",
    "#         # Common Indian cooking verbs and phrases\n",
    "#         self.cooking_verbs = [\n",
    "#             \"add\", \"stir\", \"mix\", \"cook\", \"heat\", \"boil\", \"simmer\", \"fry\", \"roast\", \"toast\",\n",
    "#             \"grind\", \"blend\", \"pressure cook\", \"sauté\", \"temper\", \"garnish\", \"marinate\", \n",
    "#             \"knead\", \"ferment\", \"steam\", \"strain\"\n",
    "#         ]\n",
    "        \n",
    "#         # Cooking connectors and transitions\n",
    "#         self.connectors = [\n",
    "#             \"then\", \"next\", \"after that\", \"meanwhile\", \"now\", \"finally\", \"allow to\", \n",
    "#             \"continue to\", \"once\", \"when\"\n",
    "#         ]\n",
    "        \n",
    "#         # Template structures for recipe steps\n",
    "#         self.step_templates = [\n",
    "#             \"First, {verb} the {ingredient} {adverb}.\",\n",
    "#             \"{Verb} the {ingredient} until {condition}.\",\n",
    "#             \"{Connector}, {verb} the {ingredient} with {ingredient2}.\",\n",
    "#             \"{Verb} {amount} of {ingredient} and {verb2} it {adverb}.\",\n",
    "#             \"In a {utensil}, {verb} the {ingredient} for {time}.\",\n",
    "#             \"{Verb} the {ingredient} {adverb}, then {verb2} in the {ingredient2}.\",\n",
    "#             \"{Connector} {verb} the mixture until it {condition}.\",\n",
    "#             \"Add the {ingredient} and {verb} for another {time}.\",\n",
    "#             \"{Verb} the {ingredient} and set aside.\",\n",
    "#             \"Garnish with {ingredient} and serve {condition}.\"\n",
    "#         ]\n",
    "        \n",
    "#         # Common cooking utensils\n",
    "#         self.utensils = [\n",
    "#             \"pan\", \"pot\", \"kadai\", \"tawa\", \"pressure cooker\", \"bowl\", \"dish\", \n",
    "#             \"skillet\", \"wok\", \"mixer\", \"grinder\"\n",
    "#         ]\n",
    "        \n",
    "#         # Cooking conditions\n",
    "#         self.conditions = [\n",
    "#             \"golden brown\", \"soft\", \"cooked through\", \"fragrant\", \"thick\", \n",
    "#             \"well combined\", \"translucent\", \"tender\", \"crispy\", \"hot\", \"warm\"\n",
    "#         ]\n",
    "        \n",
    "#         # Cooking adverbs\n",
    "#         self.adverbs = [\n",
    "#             \"gently\", \"thoroughly\", \"carefully\", \"continuously\", \"occasionally\", \n",
    "#             \"frequently\", \"slowly\", \"quickly\", \"evenly\", \"well\"\n",
    "#         ]\n",
    "        \n",
    "#         # Cooking times\n",
    "#         self.times = [\n",
    "#             \"2-3 minutes\", \"5 minutes\", \"10 minutes\", \"15-20 minutes\", \n",
    "#             \"30 minutes\", \"an hour\", \"a few seconds\"\n",
    "#         ]\n",
    "        \n",
    "#         # Amounts\n",
    "#         self.amounts = [\n",
    "#             \"1 tablespoon\", \"2 tablespoons\", \"1 teaspoon\", \"1/2 cup\", \"1 cup\", \n",
    "#             \"a handful\", \"a pinch\", \"a small amount\"\n",
    "#         ]\n",
    "    \n",
    "#     def extract_ingredients_from_text(self, text):\n",
    "#         \"\"\"Extract potential ingredients from the damaged text\"\"\"\n",
    "#         # Lowercase and replace periods with spaces\n",
    "#         text = text.lower().replace('.', ' ')\n",
    "#         words = text.split()\n",
    "        \n",
    "#         # Find potential ingredients\n",
    "#         potential_ingredients = []\n",
    "        \n",
    "#         # First check for known ingredients\n",
    "#         for ingredient in self.indian_ingredients:\n",
    "#             if ingredient in text:\n",
    "#                 potential_ingredients.append(ingredient)\n",
    "        \n",
    "#         # Then look for potential ingredients by examining each word\n",
    "#         for word in words:\n",
    "#             word = word.strip(',.;:')\n",
    "#             # Skip very short words and common cooking verbs/connectors\n",
    "#             if (len(word) > 3 and \n",
    "#                 word not in self.cooking_verbs and \n",
    "#                 word not in self.connectors and\n",
    "#                 not any(word in ing for ing in potential_ingredients)):\n",
    "#                 potential_ingredients.append(word)\n",
    "        \n",
    "#         # Limit to reasonable number\n",
    "#         random.shuffle(potential_ingredients)\n",
    "#         return potential_ingredients[:8]  # Limit to 8 ingredients\n",
    "    \n",
    "#     def extract_verbs_from_text(self, text):\n",
    "#         \"\"\"Extract potential cooking verbs from the damaged text\"\"\"\n",
    "#         text = text.lower()\n",
    "#         found_verbs = []\n",
    "        \n",
    "#         # Look for standard cooking verbs\n",
    "#         for verb in self.cooking_verbs:\n",
    "#             if verb in text:\n",
    "#                 found_verbs.append(verb)\n",
    "        \n",
    "#         # If we found at least 3 verbs, use those; otherwise use default verbs\n",
    "#         if len(found_verbs) >= 3:\n",
    "#             return found_verbs\n",
    "#         else:\n",
    "#             return self.cooking_verbs\n",
    "    \n",
    "#     def identify_cuisine_from_text(self, text):\n",
    "#         \"\"\"Try to identify the cuisine type from the damaged text\"\"\"\n",
    "#         text = text.lower()\n",
    "#         cuisine_scores = defaultdict(int)\n",
    "        \n",
    "#         # Score each cuisine based on matching dishes\n",
    "#         for cuisine, dishes in self.indian_dishes.items():\n",
    "#             for dish in dishes:\n",
    "#                 if dish in text:\n",
    "#                     cuisine_scores[cuisine] += 2\n",
    "            \n",
    "#         # If no clear winner, try matching on ingredients characteristic of regions\n",
    "#         if not cuisine_scores or max(cuisine_scores.values()) < 2:\n",
    "#             # Simple regional ingredient indicators\n",
    "#             regional_indicators = {\n",
    "#                 \"North Indian\": [\"paneer\", \"ghee\", \"cream\", \"butter\", \"rajma\", \"chole\"],\n",
    "#                 \"South Indian\": [\"sambhar\", \"idli\", \"dosa\", \"coconut\", \"rasam\", \"curry leaves\"],\n",
    "#                 \"Bengali\": [\"fish\", \"mustard\", \"posto\", \"mishti\", \"ilish\", \"chingri\"],\n",
    "#                 \"Gujarati\": [\"dhokla\", \"fafda\", \"jaggery\", \"besan\", \"kadhi\"],\n",
    "#                 \"Maharashtrian\": [\"vada\", \"misal\", \"poha\", \"kokum\", \"goda masala\"],\n",
    "#                 \"Hyderabadi\": [\"biryani\", \"haleem\", \"salan\", \"keema\"]\n",
    "#             }\n",
    "            \n",
    "#             for cuisine, indicators in regional_indicators.items():\n",
    "#                 for indicator in indicators:\n",
    "#                     if indicator in text:\n",
    "#                         cuisine_scores[cuisine] += 1\n",
    "        \n",
    "#         # Return most likely cuisine or default\n",
    "#         if cuisine_scores:\n",
    "#             return max(cuisine_scores.items(), key=lambda x: x[1])[0]\n",
    "#         else:\n",
    "#             return \"Indian\"  # Default\n",
    "    \n",
    "#     def reconstruct_recipe(self, broken_text, dish_name=None):\n",
    "#         \"\"\"\n",
    "#         Transform severely broken recipe text into coherent instructions\n",
    "#         using both the input text and template-based reconstruction.\n",
    "#         \"\"\"\n",
    "#         # Extract usable content from broken text\n",
    "#         ingredients = self.extract_ingredients_from_text(broken_text)\n",
    "#         verbs = self.extract_verbs_from_text(broken_text)\n",
    "#         cuisine = self.identify_cuisine_from_text(broken_text)\n",
    "        \n",
    "#         # If dish name not provided, try to identify it or create generic one\n",
    "#         if not dish_name:\n",
    "#             # Try to find a dish name in the text\n",
    "#             for dishes in self.indian_dishes.values():\n",
    "#                 for dish in dishes:\n",
    "#                     if dish in broken_text.lower():\n",
    "#                         dish_name = dish\n",
    "#                         break\n",
    "#                 if dish_name:\n",
    "#                     break\n",
    "            \n",
    "#             # If still no dish name, create one from primary ingredients\n",
    "#             if not dish_name and ingredients:\n",
    "#                 primary_ingredient = ingredients[0]\n",
    "#                 dish_types = [\"curry\", \"masala\", \"fry\", \"roast\", \"stir-fry\", \"bhaji\"]\n",
    "#                 dish_name = f\"{primary_ingredient} {random.choice(dish_types)}\"\n",
    "        \n",
    "#         # Create a coherent recipe\n",
    "#         reconstructed_recipe = f\"# {dish_name.title()}\\n\\n\"\n",
    "        \n",
    "#         # Add ingredients section\n",
    "#         reconstructed_recipe += \"## Ingredients\\n\\n\"\n",
    "#         for ingredient in ingredients:\n",
    "#             amount = random.choice(self.amounts)\n",
    "#             reconstructed_recipe += f\"- {amount} {ingredient}\\n\"\n",
    "        \n",
    "#         # Add spices section\n",
    "#         spices = random.sample(self.indian_ingredients[:12], min(5, len(self.indian_ingredients[:12])))\n",
    "#         reconstructed_recipe += \"- Spices (to taste): \" + \", \".join(spices) + \"\\n\\n\"\n",
    "        \n",
    "#         # Add instructions section\n",
    "#         reconstructed_recipe += \"## Instructions\\n\\n\"\n",
    "        \n",
    "#         # Generate 4-6 coherent steps\n",
    "#         num_steps = random.randint(4, 6)\n",
    "        \n",
    "#         # Create a logical flow of steps\n",
    "#         used_templates = set()\n",
    "#         verbs_cycle = verbs.copy()\n",
    "        \n",
    "#         for i in range(num_steps):\n",
    "#             # Ensure we don't run out of verbs\n",
    "#             if not verbs_cycle:\n",
    "#                 verbs_cycle = verbs.copy()\n",
    "                \n",
    "#             # Select template avoiding repetition when possible\n",
    "#             available_templates = [t for t in self.step_templates if t not in used_templates]\n",
    "#             if not available_templates:\n",
    "#                 available_templates = self.step_templates\n",
    "                \n",
    "#             template = random.choice(available_templates)\n",
    "#             used_templates.add(template)\n",
    "            \n",
    "#             # Fill template with appropriate content\n",
    "#             verb = random.choice(verbs_cycle)\n",
    "#             verbs_cycle.remove(verb)\n",
    "            \n",
    "#             verb2 = random.choice(verbs)\n",
    "#             ingredient = random.choice(ingredients)\n",
    "#             ingredients_remaining = [ing for ing in ingredients if ing != ingredient]\n",
    "#             ingredient2 = random.choice(ingredients_remaining) if ingredients_remaining else random.choice(ingredients)\n",
    "            \n",
    "#             # Format the step\n",
    "#             step = template.format(\n",
    "#                 verb=verb,\n",
    "#                 Verb=verb.capitalize(),\n",
    "#                 ingredient=ingredient,\n",
    "#                 ingredient2=ingredient2,\n",
    "#                 adverb=random.choice(self.adverbs),\n",
    "#                 Connector=random.choice(self.connectors).capitalize(),\n",
    "#                 connector=random.choice(self.connectors),\n",
    "#                 condition=random.choice(self.conditions),\n",
    "#                 time=random.choice(self.times),\n",
    "#                 utensil=random.choice(self.utensils),\n",
    "#                 amount=random.choice(self.amounts),\n",
    "#                 verb2=verb2\n",
    "#             )\n",
    "            \n",
    "#             reconstructed_recipe += f\"{i+1}. {step}\\n\"\n",
    "        \n",
    "#         # Add final serving instruction\n",
    "#         serve_with = random.choice([\"rice\", \"roti\", \"naan\", \"paratha\", \"bread\"])\n",
    "#         reconstructed_recipe += f\"\\nServe hot with {serve_with}. Enjoy your {dish_name}!\\n\"\n",
    "        \n",
    "#         return reconstructed_recipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053734d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Kadappa With Paratha\n",
      "\n",
      "## Ingredients\n",
      "\n",
      "- 1 tablespoon flame\n",
      "- a small amount even\n",
      "- 1 tablespoon evaporated\n",
      "- a small amount start\n",
      "- a handful first\n",
      "- 2 tablespoons paratha\n",
      "- a handful kadappa\n",
      "- 1 cup through\n",
      "- Spices (to taste): cumin, asafoetida, curry leaves, coriander, cardamom\n",
      "\n",
      "## Instructions\n",
      "\n",
      "1. Garnish with through and serve warm.\n",
      "2. First, cook the through carefully.\n",
      "3. Add the first and grind for another 5 minutes.\n",
      "4. Grind 1 cup of first and cook it slowly.\n",
      "\n",
      "Serve hot with rice. Enjoy your Kadappa with Paratha!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Initialize it\n",
    "# reconstructor = RecipeTextReconstructor()\n",
    "\n",
    "# # Process your model's output\n",
    "# model_output = \"Tender.meanwhile through.make kadappa through.make first.preheat pulikachalsoak even.now flame.start sambhar paratha.cook evaporated.at ivy skillet.grind...\"\n",
    "\n",
    "# # Get coherent recipe (optionally provide dish name)\n",
    "# fixed_recipe = reconstructor.reconstruct_recipe(\n",
    "#     model_output, \n",
    "#     dish_name=\"Kadappa with Paratha\"  # Optional\n",
    "# )\n",
    "\n",
    "# print(fixed_recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73d65de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "# reconstructor = RecipeTextReconstructor()\n",
    "# fixed_recipe = reconstructor.reconstruct_recipe(broken_text, \"Paneer Butter Masala\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc9f7696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_recipe_instructions(input_text, encoder_model, decoder_model, \n",
    "#                                 encoder_tokenizer, decoder_tokenizer, \n",
    "#                                 reverse_decoder_word_index):\n",
    "#     \"\"\"\n",
    "#     Generate recipe instructions from input text using the improved decoding algorithm.\n",
    "#     \"\"\"\n",
    "#     # Tokenize and pad the input sequence\n",
    "#     input_seq = encoder_tokenizer.texts_to_sequences([input_text])\n",
    "#     input_seq = np.array(input_seq)\n",
    "    \n",
    "#     # Generate recipe instructions\n",
    "#     recipe = decode_sequence_improved(\n",
    "#         input_seq, \n",
    "#         encoder_model, \n",
    "#         decoder_model, \n",
    "#         decoder_tokenizer, \n",
    "#         reverse_decoder_word_index,\n",
    "#         beam_width=5,\n",
    "#         temperature=1.2,\n",
    "#         length_norm_alpha=0.7,\n",
    "#         diversity_penalty=0.3\n",
    "#     )\n",
    "    \n",
    "#     return recipe\n",
    "\n",
    "# # Example usage:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "714a62b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recipe = generate_recipe_instructions(\n",
    "#     \"chicken curry with spices and herbs\", \n",
    "#     encoder_model, \n",
    "#     decoder_model, \n",
    "#     encoder_tokenizer, \n",
    "#     decoder_tokenizer, \n",
    "#     reverse_decoder_word_index\n",
    "# )\n",
    "# print(recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79ef9195-2a39-4ef0-b8f5-3b3c1fdde624",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T10:12:39.752030Z",
     "iopub.status.busy": "2025-05-18T10:12:39.751755Z",
     "iopub.status.idle": "2025-05-18T10:12:52.617924Z",
     "shell.execute_reply": "2025-05-18T10:12:52.617340Z",
     "shell.execute_reply.started": "2025-05-18T10:12:39.752010Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Beam Width 10:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'predict_instruction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# # Beam search with width 5\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# print(\"Beam Width 5:\")\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# print(predict_instruction(\"chicken tomato onion garlic cumin\", beam_width=5))\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Beam search with width 10\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBeam Width 10:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpredict_instruction\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchicken tomato onion garlic cumin\u001b[39m\u001b[38;5;124m\"\u001b[39m, beam_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# # Greedy for comparison\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# print(\"\\nGreedy (Beam Width = 1):\")\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# print(predict_instruction(\"lamb onion garlic\", beam_width=1))\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predict_instruction' is not defined"
     ]
    }
   ],
   "source": [
    "# # Beam search with width 5\n",
    "# print(\"Beam Width 5:\")\n",
    "# print(predict_instruction(\"chicken tomato onion garlic cumin\", beam_width=5))\n",
    "\n",
    "# Beam search with width 10\n",
    "print(\"\\nBeam Width 10:\")\n",
    "print(predict_instruction(\"chicken tomato onion garlic cumin\", beam_width=10))\n",
    "\n",
    "# # Greedy for comparison\n",
    "# print(\"\\nGreedy (Beam Width = 1):\")\n",
    "# print(predict_instruction(\"lamb onion garlic\", beam_width=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d9fd1f-aafd-44c9-8837-032fb3fd671e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T09:28:10.183822Z",
     "iopub.status.busy": "2025-05-18T09:28:10.183078Z",
     "iopub.status.idle": "2025-05-18T09:28:10.612955Z",
     "shell.execute_reply": "2025-05-18T09:28:10.612067Z",
     "shell.execute_reply.started": "2025-05-18T09:28:10.183798Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c7254c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7426096,
     "sourceId": 11822130,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7439900,
     "sourceId": 11841462,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7439965,
     "sourceId": 11841567,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
